{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from PIL import ImageFont\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTrafficSigns(rootpath):\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        next(gtReader) # skip header\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            images.append(plt.imread(prefix + row[0])) # the 1th column is the filename\n",
    "            labels.append(row[7]) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "    # extract a 3D color histogram from the HSV color space using\n",
    "    # the supplied number of `bins` per channel\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins,\n",
    "        [0, 180, 0, 256, 0, 256])\n",
    "\n",
    "    # handle normalizing the histogram if we are using OpenCV 2.4.X\n",
    "    if imutils.is_cv2():\n",
    "        hist = cv2.normalize(hist)\n",
    "\n",
    "    # otherwise, perform \"in place\" normalization in OpenCV 3\n",
    "    else:\n",
    "        cv2.normalize(hist, hist)\n",
    "\n",
    "    # return the flattened histogram as the feature vector\n",
    "    return hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from npy files...\n",
      "Training images and labels are loaded in variables ==> X,y\n",
      "Number of training Images 39209 \n",
      "Number of Labels 39209\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"Image_n_Labels/trainImages.npy\") &  os.path.isfile(\"Image_n_Labels/trainLabels.npy\") :\n",
    "    print(\"Loading from npy files...\")\n",
    "    X = np.load(\"Image_n_Labels/trainImages.npy\")\n",
    "    y = np.load(\"Image_n_Labels/trainLabels.npy\")\n",
    "    print(\"Training images and labels are loaded in variables ==> X,y\")\n",
    "    print(\"Number of training Images {} \\nNumber of Labels {}\".format(len(X), len(y)))\n",
    "else:    \n",
    "    # training images and labels\n",
    "    trainImages, trainLabels = readTrafficSigns('/home/nikita/Downloads/gtsrb/dataset/GTSRB_finaltraining/Final_Training/Images/')\n",
    "    np.save(\"Image_n_Labels/trainImages.npy\",trainImages)\n",
    "    np.save(\"Image_n_Labels/trainLabels.npy\",trainLabels)\n",
    "    print(\"training images and labels are read from the dataset directory\")\n",
    "    print(\"training images saved to Image_n_Labels/trainingImages.npy for further use\")\n",
    "    print(\"training labels saved to Image_n_Labels/trainingLabels.npy for further use\")\n",
    "    X = np.load(\"Image_n_Labels/trainImages.npy\")\n",
    "    y = np.load(\"Image_n_Labels/trainLabels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 3000/39209\n",
      "[INFO] processed 6000/39209\n",
      "[INFO] processed 9000/39209\n",
      "[INFO] processed 12000/39209\n",
      "[INFO] processed 15000/39209\n",
      "[INFO] processed 18000/39209\n",
      "[INFO] processed 21000/39209\n",
      "[INFO] processed 24000/39209\n",
      "[INFO] processed 27000/39209\n",
      "[INFO] processed 30000/39209\n",
      "[INFO] processed 33000/39209\n",
      "[INFO] processed 36000/39209\n",
      "[INFO] processed 39000/39209\n"
     ]
    }
   ],
   "source": [
    "# initialize the raw pixel intensities matrix, the features matrix,\n",
    "# and labels list\n",
    "features = []\n",
    "labels = y\n",
    "# loop over the input images\n",
    "for i in range(0,len(X)):\n",
    "# for i in range(0,10):\n",
    "    image = X[i]\n",
    "    # extract raw pixel intensity \"features\", followed by a color\n",
    "    # histogram to characterize the color distribution of the pixels\n",
    "    # in the image\n",
    "    hist = extract_color_histogram(image)\n",
    "    # update the raw images, features, and labels matricies,\n",
    "    # respectively\n",
    "    features.append(hist)\n",
    "    # show an update every 3,000 images\n",
    "    if i > 0 and i % 3000 == 0:\n",
    "        print(\"[INFO] processed {}/{}\".format(i, len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] features matrix: 78.42MB\n"
     ]
    }
   ],
   "source": [
    "# show some information on the memory consumed by the raw images\n",
    "# matrix and features matrix\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "print(\"[INFO] features matrix: {:.2f}MB\".format(features.nbytes / (1024 * 1000.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data points: 26465\n",
      "validation data points: 2941\n",
      "testing data points: 9803\n"
     ]
    }
   ],
   "source": [
    "# take the  data and construct the training and testing split, using 75% of the\n",
    "# data for training and 25% for testing\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(features,\n",
    "    labels, test_size=0.25, random_state=42)\n",
    " \n",
    "# now, let's take 10% of the training data and use that for validation\n",
    "(trainData, valData, trainLabels, valLabels) = train_test_split(trainData, trainLabels,\n",
    "    test_size=0.1, random_state=84)\n",
    " \n",
    "# show the sizes of each data split\n",
    "print(\"training data points: {}\".format(len(trainLabels)))\n",
    "print(\"validation data points: {}\".format(len(valLabels)))\n",
    "print(\"testing data points: {}\".format(len(testLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, accuracy=96.70%\n",
      "k=3, accuracy=92.93%\n",
      "k=5, accuracy=89.39%\n",
      "k=7, accuracy=86.33%\n",
      "k=9, accuracy=83.99%\n",
      "k=11, accuracy=80.86%\n",
      "k=13, accuracy=78.14%\n",
      "k=15, accuracy=75.65%\n",
      "k=17, accuracy=74.33%\n",
      "k=19, accuracy=72.02%\n",
      "k=21, accuracy=70.59%\n",
      "k=23, accuracy=67.97%\n",
      "k=25, accuracy=67.12%\n",
      "k=27, accuracy=65.56%\n",
      "k=29, accuracy=63.65%\n",
      "k=1 achieved highest accuracy of 96.70% on validation data\n"
     ]
    }
   ],
   "source": [
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the\n",
    "# list of accuracies for each value of k\n",
    "kVals = range(1, 30, 2)\n",
    "accuracies = []\n",
    " \n",
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "for k in range(1, 30, 2):\n",
    "    # train the k-Nearest Neighbor classifier with the current value of `k`\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(trainData, trainLabels)\n",
    "\n",
    "    # evaluate the model and update the accuracies list\n",
    "    score = model.score(valData, valLabels)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    accuracies.append(score)\n",
    "\n",
    "# find the value of k that has the largest accuracy\n",
    "i = np.argmax(accuracies)\n",
    "print(\"k=%d achieved highest accuracy of %.2f%% on validation data\" % (kVals[i],\n",
    "    accuracies[i] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pre-trained classifier not found. \n",
      " Training Classifier \\KNN = 1\n",
      "[INFO] Succefully trained the classsifier. \n",
      " Saving the classifier for further use\n",
      "[INFO] Classifier Saved\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate a k-NN classifer on the histogram\n",
    "# representations\n",
    "# if os.path.isfile('c'\n",
    "name = \"clf/clf_knn_cv_clrhist.pkl\"\n",
    "if os.path.isfile(name):\n",
    "    print(\"[INFO] loading classifier: KNN ={} trained on color histogram features...\".format(kVals[i]))\n",
    "    model= joblib.load(name)\n",
    "    print(\"[INFO] Classifer is loaded as instance ::model::\")\n",
    "else:\n",
    "    print(\"[INFO] pre-trained classifier not found. \\n Training Classifier \\KNN = {}\".format(kVals[i]))\n",
    "    model = KNeighborsClassifier(n_neighbors=1,n_jobs=2)\n",
    "    model.fit(trainData, trainLabels)\n",
    "    print(\"[INFO] Succefully trained the classsifier. \\n Saving the classifier for further use\")\n",
    "    joblib.dump(model, name) \n",
    "    print(\"[INFO] Classifier Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION ON TESTING DATA\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.95      0.97        44\n",
      "          1       0.96      0.96      0.96       594\n",
      "         10       0.94      0.95      0.95       461\n",
      "         11       0.97      0.97      0.97       291\n",
      "         12       0.99      0.97      0.98       548\n",
      "         13       0.95      0.96      0.95       549\n",
      "         14       0.98      0.99      0.99       197\n",
      "         15       0.97      0.98      0.97       152\n",
      "         16       0.97      0.94      0.96       108\n",
      "         17       0.99      0.98      0.99       286\n",
      "         18       0.98      0.95      0.96       288\n",
      "         19       0.96      1.00      0.98        53\n",
      "          2       0.97      0.94      0.96       560\n",
      "         20       0.95      0.99      0.97        90\n",
      "         21       0.99      0.98      0.98        82\n",
      "         22       0.92      0.98      0.95       107\n",
      "         23       0.98      0.99      0.99       138\n",
      "         24       0.97      0.93      0.95        68\n",
      "         25       0.94      0.96      0.95       369\n",
      "         26       0.99      0.97      0.98       152\n",
      "         27       0.95      0.98      0.97        60\n",
      "         28       0.95      0.98      0.96       129\n",
      "         29       0.92      0.95      0.94        77\n",
      "          3       0.95      0.94      0.95       348\n",
      "         30       0.95      0.97      0.96       118\n",
      "         31       0.95      0.99      0.97       201\n",
      "         32       1.00      0.96      0.98        48\n",
      "         33       0.97      0.99      0.98       182\n",
      "         34       0.99      0.97      0.98       117\n",
      "         35       0.97      0.98      0.97       290\n",
      "         36       0.98      0.92      0.95       105\n",
      "         37       0.98      0.98      0.98        60\n",
      "         38       0.98      0.98      0.98       517\n",
      "         39       0.97      1.00      0.99        69\n",
      "          4       0.95      0.96      0.95       529\n",
      "         40       0.93      0.96      0.94        77\n",
      "         41       0.95      0.98      0.97        60\n",
      "         42       0.98      0.95      0.96        56\n",
      "          5       0.97      0.96      0.96       462\n",
      "          6       0.97      0.96      0.97        81\n",
      "          7       0.97      0.97      0.97       336\n",
      "          8       0.98      0.97      0.98       385\n",
      "          9       0.96      0.96      0.96       359\n",
      "\n",
      "avg / total       0.97      0.97      0.97      9803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testData)\n",
    "# show a final classification report demonstrating the accuracy of the classifier\n",
    "\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(testLabels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
